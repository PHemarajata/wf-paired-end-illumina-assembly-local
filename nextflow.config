/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    bacterial-genomics/wf-paired-end-illumina-assembly Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

/*
================================================================================
    Load configuration files
================================================================================
*/

// Load configuration files
includeConfig "conf/params.config"
includeConfig "conf/base.config"
includeConfig "conf/workflows.config"
includeConfig "conf/modules.config"

// Set an empty test profile to pass `nf-core lint`
// This will be replaced when the profiles.conf file is loaded
// profiles { test{} }

// Load nf-core custom profiles from different Institutions
// try {
//     includeConfig "${params.custom_config_base}/nfcore_custom.config"
// } catch (Exception e) {
//     System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
// }

// Workflow configs
// This code is adapted from: https://github.com/bactopia/bactopia
if (params.workflows.containsKey(params.assembler)) {
    if (params.workflows[params.assembler].containsKey("is_workflow")) {
        // Main workflow
        params.workflows[params.assembler]['includes'].each { it ->
            if (params.workflows[it].containsKey("modules")) {
                // Subworkflows
                params.workflows[it]['modules'].each { module ->
                    includeConfig "${params.workflows[module].path}/params.config"
                }
            } else {
                // For each Module
                includeConfig "${params.workflows[it].path}/params.config"
            }
        }
    }
}

// Load profiles after modules/params.conf are loaded or else test profile won't work
//includeConfig "conf/profiles.config"
profiles {
    // nf-core profiles
    debug {
        process.beforeScript   = 'echo $HOSTNAME'
    }

    conda {
        params.enable_conda    = true
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        conda.cacheDir         = "${params.profile_cache_dir}"
        includeConfig "conf/base.config"
    }

    docker {
        docker.enabled         = true
        singularity.enabled    = false
        shifter.enabled        = false
        fixOwnership           = true
        runOptions             = "-u \$(id -u):\$(id -g)"
        docker.cacheDir        = "${params.profile_cache_dir}"
        includeConfig "conf/base.config"
    }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        shifter.enabled        = false
        singularity.cacheDir   = "${params.profile_cache_dir}"
        includeConfig "conf/base.config"
    }

    shifter {
        shifter.enabled        = true
        docker.enabled         = false
        singularity.enabled    = false
        includeConfig "conf/base.config"
    }
  gcb {
    // — executor + staging bucket
    process.executor = 'google-batch'
    workDir          = 'gs://aphlhq-ngs-gh/nextflow_work'
    includeConfig "conf/base.config"
    batch.volumes = [ 'gs://aphlhq-ngs-gh/': '/mnt/gcs' ]
    batch.logsPolicy = 'ENABLED'
    // — your pipeline parameters
   
    // — GCP settings
    google {
        project  = 'erudite-pod-307018'
        region = 'us-central1'
        options = '--labels=workflow=assembly,env=dev'
    }

    // ++ ADD THIS ENTIRE BLOCK ++
    // -- Process resources for Google Cloud Batch --
    process {
        // Default resources for any process without a specific label
        cpus   = 2
        memory = '8.GB'

        // Resources for medium-sized jobs
        withLabel: 'process_medium' {
            cpus   = 6
            memory = '24.GB'
        }

        // Resources for large jobs
        withLabel: 'process_high' {
            cpus   = 16
            memory = '64.GB'
        }

        // Add more labels as needed, for example:
        withLabel: 'process_long' {
            time = '24.h'
        }
    }
}
    // Aspen Univa Grid Engine profile
    aspen_hpc {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        shifter.enabled        = false
        singularity.cacheDir   = "${LAB_HOME}/workflows/singularity.cache"
        includeConfig "conf/base.config"
        includeConfig "conf/profiles/aspen_hpc.config"
    }

    // Rosalind Univa Grid Engine profile
    rosalind_hpc {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        shifter.enabled        = false
        singularity.cacheDir   = "${LAB_HOME}/workflows/singularity.cache"
        includeConfig "conf/base.config"
        includeConfig "conf/profiles/rosalind_hpc.config"
    }

    // Main test profiles
    test      { includeConfig 'conf/test.config'      }
    test_full { includeConfig 'conf/test_full.config' }

    // Specialized test profiles
    test_cat_checkm2_very_high_cpu_local {
        includeConfig 'conf/test_cat_checkm2_very_high_cpu_local.config'
    }
    
    // Working Docker profile that fixes bash_functions.sh issue
    docker_working {
        includeConfig 'conf/docker_working.config'
    }
    
    // Optimized profiles for local high-performance workstation (22 cores, 64GB RAM)
    optimized_docker {
        includeConfig 'conf/optimized_local.config'
        params.use_singularity = false
        docker.enabled         = true
        docker.userEmulation   = true
        conda.enabled          = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    
    optimized_singularity {
        includeConfig 'conf/optimized_local.config'
        params.use_singularity = true
        singularity.enabled    = true
        singularity.autoMounts = true
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
}
/*
================================================================================
    Workflow information
================================================================================
*/

manifest {
    name            = 'bacterial-genomics/wf-paired-end-illumina-assembly'
    author          = "Christopher A. Gulvik"
    homePage        = 'https://github.com/bacterial-genomics/wf-paired-end-illumina-assembly'
    description     = "Clean, assemble, and annotate paired end illumina reads."
    mainScript      = 'main.nf'
    nextflowVersion = '!>=22.04.3'
    version         = '3.0.0'
}

/*
================================================================================
    Nextflow output files
================================================================================
*/

// Function to get current timestamp
def trace_timestamp = new java.util.Date().format('yyyy-MMM-dd_EEE_HH-mm-ss')

timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.html"
}

/*
================================================================================
    Environment variables
================================================================================
*/

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
    JULIA_DEPOT_PATH = "/usr/local/share/julia"
}

/*
================================================================================
    Workflow functions
================================================================================
*/

// Function to ensure that resource requirements don't go beyond a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min(obj, params.max_cpus as int)
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
